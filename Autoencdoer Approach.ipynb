{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from PIL import Image\n",
    "from scipy.misc import imsave\n",
    "import glob\n",
    "from os.path import basename\n",
    "from scipy.misc import imresize\n",
    "\n",
    "\n",
    "from keras import Model\n",
    "from keras.models import load_model\n",
    "from keras.layers import Conv2D, Input, Dense,Flatten,LeakyReLU, BatchNormalization, MaxPooling2D, UpSampling2D,Activation\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "print(K.backend())\n",
    "\n",
    "from image_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train_data, augmented_train_label = load_augmented_data(\"./augmented_data/\", \"train\")\n",
    "augmented_validation_data, augmented_validation_label = load_augmented_data(\"./augmented_data/\", \"val\")\n",
    "augmented_test_data, augmented_test_label = load_augmented_data(\"./augmented_data/\", \"test\")\n",
    "\n",
    "partition_dict, labels_dict = get_generator_dicts(\"./augmented_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luca\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  \n",
      "C:\\Users\\Luca\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\Luca\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  import sys\n",
      "C:\\Users\\Luca\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "resize_factor = 0.6\n",
    "augmented_train_data_resized = np.zeros((augmented_train_data.shape[0], imresize(augmented_train_data[0], resize_factor).shape[0], imresize(augmented_train_data[0], resize_factor).shape[1], 3))\n",
    "augmented_validation_data_resized = np.zeros((augmented_validation_data.shape[0], imresize(augmented_validation_data[0], resize_factor).shape[0], imresize(augmented_validation_data[0], resize_factor).shape[1], 3))\n",
    "\n",
    "\n",
    "for i, im in enumerate(augmented_train_data):\n",
    "    augmented_train_data_resized[i] = imresize(im, resize_factor)/255\n",
    "    \n",
    "\n",
    "for i, im in enumerate(augmented_validation_data):\n",
    "    augmented_validation_data_resized[i] = imresize(im, resize_factor)/255\n",
    "    #plot_with_label(augmented_validation_data_resized[i], augmented_validation_label[i][0], augmented_validation_label[i][1])\n",
    "    \n",
    "augmented_train_data = augmented_train_data_resized\n",
    "augmented_validation_data = augmented_validation_data_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luca\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  \n",
      "C:\\Users\\Luca\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "augmented_train_data_resized_target = np.zeros((augmented_train_data.shape[0], 200, 296, 3))\n",
    "augmented_validation_data_resized_target = np.zeros((augmented_validation_data.shape[0], 200, 296, 3))\n",
    "\n",
    "\n",
    "for i, im in enumerate(augmented_train_data):\n",
    "    augmented_train_data_resized_target[i] = imresize(im, (200, 296))/255\n",
    "    \n",
    "\n",
    "for i, im in enumerate(augmented_validation_data):\n",
    "    augmented_validation_data_resized_target[i] = imresize(im, (200, 296))/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, path, list_IDs, labels, batch_size=32, dim=(32,32,32), n_channels=3,\n",
    "                  shuffle=True, fully_convolutional = True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        self.path = path\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size, 2), dtype=float)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = imresize(np.array(Image.open(self.path + ID + '.png')), resize_factor)/255\n",
    "            \n",
    "\n",
    "            # Store class\n",
    "            y[i,0], y[i,1] = self.labels[ID]\n",
    "            \n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbCallBack = TensorBoard(log_dir='./logs/regression', histogram_freq=2, batch_size=32, write_graph=True, write_grads=True, write_images=True)\n",
    "cpCallBack = ModelCheckpoint('./checkpoints/regression_auto/weights-improvement-{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=3)\n",
    "esCallBack = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "\n",
    "K.clear_session()\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 195, 294, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 195, 294, 64) 1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 195, 294, 64) 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 195, 294, 64) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 98, 147, 64)  0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 98, 147, 32)  18464       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 98, 147, 32)  128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 98, 147, 32)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 49, 74, 32)   0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 49, 74, 16)   4624        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 49, 74, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 49, 74, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 25, 37, 16)   0           leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 25, 37, 16)   2320        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 25, 37, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 25, 37, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 50, 74, 16)   0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 50, 74, 32)   4640        up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 50, 74, 32)   128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 50, 74, 32)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 100, 148, 32) 0           leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 100, 148, 64) 18496       up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 100, 148, 64) 256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 100, 148, 64) 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 200, 296, 64) 0           leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 14800)        0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 200, 296, 3)  1731        up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 8)            118400      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 200, 296, 3)  12          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Dense)             (None, 2)            16          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "aux_output (Activation)         (None, 200, 296, 3)  0           batch_normalization_7[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 171,391\n",
      "Trainable params: 170,937\n",
      "Non-trainable params: 454\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_img = Input(shape=(augmented_train_data[0].shape[0], augmented_train_data[0].shape[1], 3))\n",
    "\n",
    "x = Conv2D(64, (3, 3), padding='same')(input_img)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(32, (3, 3), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(16, (3, 3), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "x = Conv2D(16, (3, 3), padding='same')(encoded)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(64, (3, 3), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(3, (3, 3), padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "decoded = Activation('sigmoid', name=\"aux_output\")(x)\n",
    "\n",
    "xx = Flatten()(encoded)\n",
    "xx = Dense(8, activation='relu', use_bias=False)(xx)\n",
    "output = Dense(2, use_bias=False, name=\"main_output\")(xx)\n",
    "\n",
    "\n",
    "\n",
    "model = Model(inputs=input_img, outputs=[output, decoded])\n",
    "\n",
    "from keras import optimizers\n",
    "#sgd = optimizers.SGD(lr=1e-11, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=1e-9)\n",
    "model.compile(optimizer=\"adam\", loss=['mean_squared_error', \"binary_crossentropy\"])\n",
    "\n",
    "#tbCallBack.set_model(model)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 412 samples, validate on 80 samples\n"
     ]
    }
   ],
   "source": [
    "params = {'dim': (augmented_train_data[0].shape[0],augmented_train_data[0].shape[1]),\n",
    "          'batch_size': 32,\n",
    "          'n_channels': 3,\n",
    "          'shuffle': True}\n",
    "training_generator = DataGenerator(\"./augmented_data/data/\", partition_dict['train'], labels_dict, **params)\n",
    "validation_generator = DataGenerator(\"./augmented_data/data/\", partition_dict['val'], labels_dict, **params)\n",
    "\n",
    "#history = model.fit_generator(generator=training_generator, validation_data=[augmented_validation_data, augmented_validation_label], epochs=3, callbacks=[cpCallBack, esCallBack])\n",
    "\n",
    "\n",
    "history = model.fit(augmented_train_data, {'main_output': augmented_train_label, 'aux_output': augmented_train_data_resized_target}, validation_data=[augmented_validation_data, {'main_output': augmented_validation_label, 'aux_output': augmented_validation_data_resized_target}] ,epochs=30, batch_size=32, callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(augmented_train_data_resized[:10])\n",
    "print(predictions)\n",
    "for i, pred in enumerate(predictions):\n",
    "    plot_with_label(augmented_train_data_resized[i], pred[0], pred[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(augmented_validation_data_resized)\n",
    "for i, pred in enumerate(predictions):\n",
    "    plot_with_label(augmented_validation_data_resized[i], pred[0], pred[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
